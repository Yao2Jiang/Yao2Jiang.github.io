<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="week5"><meta name="keywords" content><meta name="author" content="Conglei Xu"><meta name="copyright" content="Conglei Xu"><title>week5 | Welcome to my Blog.</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Overview"><span class="toc-number">1.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Basic-Knowledge-in-Reinforcement-Learning"><span class="toc-number">2.</span> <span class="toc-text">Basic Knowledge in Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-Concepts"><span class="toc-number">2.1.</span> <span class="toc-text">Basic Concepts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#value-based-methods"><span class="toc-number">2.2.</span> <span class="toc-text">value-based methods</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/yao2jiang/blog_images/master/bolg_index/new.jpg"></div><div class="author-info__name text-center">Conglei Xu</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/xiaojianhai">Follow me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">21</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">10</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">11</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yao2jiang/blog_images/master/bolg_index/header.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Welcome to my Blog.</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">week5</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-09-21</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/week-summary/">week summary</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h1 id="Basic-Knowledge-in-Reinforcement-Learning"><a href="#Basic-Knowledge-in-Reinforcement-Learning" class="headerlink" title="Basic Knowledge in Reinforcement Learning"></a>Basic Knowledge in Reinforcement Learning</h1><h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><p>** Environment： **</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><ol>
<li>reward function: give a feedback to the agent’s action.</li>
<li>transitional probability: give a new state based on the current state and agent’s action.<br>transition step: for a state $ s $, the agent choose to take an action, ant then the environment gives<br>a new state $ \acute{a} $ and a reward r. this known as a transition step, represented by a tuple $ (s, a, \acute{a}, r) $.</li>
</ol>
<p>we use $ \mathbb{P} $ represent the probability of a transiton step, $ P $ denote the function of a transition from $ s $ to<br>$ \acute{s} $.</p>
<p>$$ P(\acute{s}, r | a, s) = \mathbb{P}[ S_{t+1}={\acute{s}, r | S_t=s, a } ] $$</p>
<p>$$ P(\acute{s}|a, s) = \sum_{r} {\mathbb{P}[S_{t+1}={\acute{s}, r | S_t=s, a}]} $$</p>
<p>The reward function $ R $ could be either determinsitc(the reward is always the same) or stochastic(the reward vary according to some probability<br>distribution).</p>
<p>** Agent: **</p>
<p>agent take actions based on policy and then get future reward for this policy. The goal of RL is to find the optimical policy<br>to maximize the future reward. </p>
<p>policy tells the agent which action to take in state s. It is mapping from state s to action a.<br>It can be either determinsitc or stochastic:</p>
<ul>
<li>determinsitc: $ \pi(s) = a $ </li>
<li>stochastic: $ \pi(s) = \mathbb{P}[A=a|S=s] $</li>
</ul>
<p>the state-value of a state s is the expected return if we are in this state at time t, $ S_t = s $.</p>
<p>$$ V_{\pi}(s) = E[G_t|S_t=s] $$</p>
<p>the action value of a state-action pair:</p>
<p>$$ Q_{\pi}(s,a)=E[ G_t | S_t=s, A_t=a ] $$</p>
<p>$$ V_{\pi}(s) = \sum_{a}{Q_{\pi}(s, a)\pi(a, s)} $$</p>
<h2 id="value-based-methods"><a href="#value-based-methods" class="headerlink" title="value-based methods"></a>value-based methods</h2><p>In value-based methods, the goal is to mininze the loss between predicted value and target value to approximate true<br>action-value function. we could get the policy form optimical action-value function implicitly.</p>
<p>$$ Q_{\pi}^{\ast}=max_{\pi}Q_{\pi} $$ </p>
<p>$$ \pi^{\ast}=\mathop{argmax}\limits_{\pi}Q_{\pi} $$</p>
<p>** Bellman Expections Equations **</p>
<p>$$ V_{\pi}(s) = \sum_{a}Q_{\pi}(s,a)\pi(a|s) $$</p>
<p>$$ Q_{\pi}(s,a) = R(s, a) + \lambda \sum_{\acute{s}} \mathbb{P}<em>{s,\acute{s}}^{a}V</em>{\pi}(\acute{s}) $$ </p>
<p>$$ V_{\pi}(s) = \sum_{a}\pi(a|s)(R(s, a) + \lambda \sum_{\acute{s}}  \mathbb{P}<em>{s,\acute{s}}^{a}V</em>{\pi}(\acute{s}) $$</p>
<p>$$ Q_{\pi}(s,a) = R(s, a) + \lambda \sum_{\acute{s}} \mathbb{P}<em>{s,\acute{s}}^a(\sum</em>{a}Q_{\pi}(\acute{s},a)\pi(a|\acute{s})) $$</p>
<p>** Bellman Optimaility Equations **</p>
<p>$$ V^{\ast} = max_{a} Q^{\ast}(s,a) $$ </p>
<p>$$ Q^{\ast| = R(s,a) + \lambda \sum_{\acute{s}} \mathbb{P}_{s,\acute{s}}^{a}V^{\ast}(\acute{s}) $$</p>
<p>$$ V^{\ast} = max_{a} (R(s, a) + \lambda \sum_{\acute{s}} \mathbb{P}_{s,\acute{s}}^{a}V^{\ast}(\acute{s})) $$</p>
<p>$$ Q^{\ast}(s,a) = R(s,a) + \lambda \mathbb{P}<em>{s,\acute{s}}^amax</em>{a}Q_(\acute{s},a)\pi(a|\acute{s}) $$</p>
<p>** Dynamic Progamming **</p>
<p>In problems can be solved by dynamic progamming. the DP process can be seen as a RL process to achieve optimical Bellman Equations.</p>
<p>There ar two primary DP methods: Policy Iteration, Value Iteration.</p>
<p>** Policy Iteration **</p>
<p>** Intialization **</p>
<p>Start with an arbitiary policy ${pi}$ and arbitiary value function $ V $.</p>
<p>** Policy Evaluation **</p>
<p>Given a policy $ {pi} $, compute its value function $ V^{\pi} $. This involves solving the Bellman Expections Equations<br>for the current policy until convergence.</p>
<p>$$ V_{\pi}(s) = \sum_{a}\pi(a|s)(R(s, a) + \lambda \sum_{\acute{s}}  \mathbb{P}<em>{s,\acute{s}}^{a}V</em>{\pi}(\acute{s}) ) $$</p>
<p>** Policy Improvement **</p>
<p>Now, we have the value function $ V^{\pi} $ for the current policy, we can attempt improve the policy. For each state $ s $, select the<br>action a that maxmize the expected value.</p>
<p>$$ \acute{\pi} = argmax_{a} (R(s, a) + \lambda \sum_{\acute{s}}  \mathbb{P}<em>{s,\acute{s}}^{a}V</em>{\pi}(\acute{s}) ) $$</p>
<p>The new policy is guranteed to be at least as good as the old policy $ \pi $. we can stop the Iteration if they are same.</p>
<p>** Iteration **</p>
<p>Repeat steps 2 and stpes 3 until the policy remains unchanged between two concective Improvements.</p>
</div></article><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2023/10/18/LLMs-tuning/"><i class="fa fa-chevron-left">  </i><span>Conclusion for Fine-Tuning on LLMs</span></a></div><div class="next-post pull-right"><a href="/2023/09/04/week4/"><span>week4</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/yao2jiang/blog_images/master/bolg_index/header.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2023 By Conglei Xu</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://xiaojianhai.github.io/">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>