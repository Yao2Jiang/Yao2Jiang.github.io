<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Byte-Pair Encoding Algorithm"><meta name="keywords" content="Large Language Models"><meta name="author" content="Conglei Xu"><meta name="copyright" content="Conglei Xu"><title>Byte-Pair Encoding Algorithm | Welcome to my Blog.</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BPE-in-NLP"><span class="toc-number">2.</span> <span class="toc-text">BPE in NLP</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Practices-of-BPE"><span class="toc-number">3.</span> <span class="toc-text">Practices of BPE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BPE-in-Transformers"><span class="toc-number">3.1.</span> <span class="toc-text">BPE in Transformers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BPE-in-sentencepiece"><span class="toc-number">3.2.</span> <span class="toc-text">BPE in sentencepiece</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#References"><span class="toc-number">4.</span> <span class="toc-text">References:</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/yao2jiang/blog_images/master/bolg_index/new.jpg"></div><div class="author-info__name text-center">Conglei Xu</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/xiaojianhai">Follow me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">20</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">10</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">11</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yao2jiang/blog_images/master/bolg_index/header.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Welcome to my Blog.</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Byte-Pair Encoding Algorithm</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-11-04</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Natural-Language-Processing/">Natural Language Processing</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Byte-Pair Encoding(BPE) encoding algorithm is one of the most popular subword-tokenization methonds used in Large Language Models(LLMs)<br>such like GPT-4. By doing this, the most common words and char-patterns(eg.. sub, de, en ) will be represented as a single token, the rare words<br>are broken down into more subword tokens. It is a good methonds to decreae the number of tokens in vocabulary and learn the meaning of word morphology. </p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>BPE was proposed as a data compression algorithm, in which the most common consective bytes pair is replaced with a new sympbol.</p>
<p>Suppose we have aaabdaaabac, Fisrt, we identify the most common consective pair: ‘aa’, and then we replace ‘aa’ with a unseen sympbol Z. the string is turned into<br>‘ZabdZabac’. the next most common pair is ‘ab’. similaly, we replace ‘ab’ with a unseen sympbol ‘Y’. the string is ‘ZYdZYac’ now.<br>we repeat the replace operation until all consective pairs don’t occur more than once. </p>
<h1 id="BPE-in-NLP"><a href="#BPE-in-NLP" class="headerlink" title="BPE in NLP"></a>BPE in NLP</h1><p>The steps of BPE in NLP in English are as following:</p>
<p>First we construct a frequecy dictionary to store the frequecy of words, supposed we have vocab = {‘low/w’: 5, ‘lower/w’: 2, ‘newest/w’: 6, ‘wildest/w’: 3}, by pretokenizaton and andding the word boundries.<br>we get the initialize vocabulary by seeing each sympbol as a unit:</p>
<table>
<thead>
<tr>
<th>tokens</th>
<th>count</th>
</tr>
</thead>
<tbody><tr>
<td>e</td>
<td>17</td>
</tr>
<tr>
<td>w</td>
<td>16</td>
</tr>
<tr>
<td></td>
<td>16</td>
</tr>
<tr>
<td>l</td>
<td>10</td>
</tr>
<tr>
<td>s</td>
<td>9</td>
</tr>
<tr>
<td>t</td>
<td>9</td>
</tr>
<tr>
<td>o</td>
<td>7</td>
</tr>
<tr>
<td>n</td>
<td>6</td>
</tr>
<tr>
<td>i</td>
<td>3</td>
</tr>
<tr>
<td>d</td>
<td>3</td>
</tr>
<tr>
<td>r</td>
<td>2</td>
</tr>
</tbody></table>
<p>Then we find the most common consective pair in vocab above by:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pairs</span><span class="params">(vocabulary)</span>:</span></span><br><span class="line">    pairs = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocabulary.items():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word)<span class="number">-1</span>):</span><br><span class="line">            pairs[word[i], [word[i]]] += freq</span><br><span class="line">        pairs[word[<span class="number">-1</span>], <span class="string">'&lt;/w&gt;'</span>] += <span class="number">1</span></span><br><span class="line">    most_common = max(pairs, key=pairs.get)</span><br><span class="line">    <span class="keyword">return</span> pairs, most_common, pairs[most_common]</span><br></pre></td></tr></table></figure>

<p>we merger the most common token into initialize tokens, and recalute the fequency of consective pairs by new token set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merger_tokens</span><span class="params">(tokens_, most_common, number_occurs)</span>:</span></span><br><span class="line">    del_keys = []</span><br><span class="line">    <span class="keyword">if</span> number_occurs &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">''</span>.join(token) <span class="keyword">in</span> most_common:</span><br><span class="line">                tokens_[token] -= number_occurs</span><br><span class="line">                <span class="keyword">if</span> tokens_[token] &lt;= <span class="number">0</span>:</span><br><span class="line">                    del_keys.append(token)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> del_keys:</span><br><span class="line">            <span class="keyword">del</span> tokens_[item]</span><br><span class="line">        tokens_[most_common] = number_occurs</span><br><span class="line">    <span class="keyword">return</span> tokens_</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_vocab</span><span class="params">(vocab_, most_common)</span>:</span></span><br><span class="line">    vocab_new = &#123;&#125;</span><br><span class="line">    bi_gram = re.escape(<span class="string">' '</span>.join(most_common))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bi_gram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocab_:</span><br><span class="line">        word_new = p.sub(<span class="string">''</span>.join(most_common), word)</span><br><span class="line">        vocab_new[word_new] = vocab_[word]</span><br><span class="line">    <span class="keyword">return</span> vocab_new</span><br></pre></td></tr></table></figure>

<p>The most common pair is ‘es’, after updating, the new tokens set is shown below.</p>
<table>
<thead>
<tr>
<th>tokens</th>
<th>count</th>
</tr>
</thead>
<tbody><tr>
<td>w</td>
<td>16</td>
</tr>
<tr>
<td></td>
<td>16</td>
</tr>
<tr>
<td>l</td>
<td>10</td>
</tr>
<tr>
<td>es</td>
<td>9</td>
</tr>
<tr>
<td>t</td>
<td>9</td>
</tr>
<tr>
<td>e</td>
<td>8</td>
</tr>
<tr>
<td>o</td>
<td>7</td>
</tr>
<tr>
<td>n</td>
<td>6</td>
</tr>
<tr>
<td>i</td>
<td>3</td>
</tr>
<tr>
<td>d</td>
<td>3</td>
</tr>
<tr>
<td>r</td>
<td>2</td>
</tr>
</tbody></table>
<p>we repeat get_pairs and merger_tokens, replace_vocab until the words cann’t be splited anymore or the frequecy of all token reach to one.</p>
<p>BEP algorithm can extend to a cross Language encoding algorihm, by seeing a byte as a unit. below is an example of Llama:</p>
<p><a href="https://github.com/Yao2Jiang/blog_images/tree/master/pictures/LLMs_tuning/llama_tokenization.png" target="_blank" rel="noopener">Fig1. tokenization by BPE on Chinese</a></p>
<p>for Chinese, a hanzi might cut to many subtokens in Llama; it makes the tokenization of Chinese sentence longer than original sentence.</p>
<h1 id="Practices-of-BPE"><a href="#Practices-of-BPE" class="headerlink" title="Practices of BPE"></a>Practices of BPE</h1><h2 id="BPE-in-Transformers"><a href="#BPE-in-Transformers" class="headerlink" title="BPE in Transformers"></a>BPE in Transformers</h2><p>Below is the code to train a BPE model in HuggingFace transformers, we will get two files for tokenization class:<br>vocabulary.json and merger.txt. the First map word to ID and the seocnd is a prior list defining the mergeing operation prority. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WordPeace_tokenizer</span><span class="params">()</span>:</span></span><br><span class="line">    tokenizer_ = tokenizers.Tokenizer(models.WordPiece(unk_token=[<span class="string">'UNK'</span>]))</span><br><span class="line">    <span class="comment"># adding normalizer to clean text firstly</span></span><br><span class="line">    tokenizer_.normalizer = normalizers.BertNormalizer(lowercase=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># tokenizer_.normalizer = normalizers.Sequence([fun1, fun2, fun3])</span></span><br><span class="line">    tokenizer_.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br><span class="line">    <span class="comment"># tokenizer_.pre_tokenizer = pre_tokenizers.Whitespace()</span></span><br><span class="line">    <span class="comment"># tokenizer_.pre_tokenizer = pre_tokenizers.Sequence([])</span></span><br><span class="line">    <span class="comment"># passing the special tokens</span></span><br><span class="line">    special_tokens = [<span class="string">'[UNK]'</span>, <span class="string">'[PAD]'</span>, <span class="string">'[CLS]'</span>, [<span class="string">'SEP'</span>], [<span class="string">'MASK'</span>]]</span><br><span class="line">    trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br><span class="line">    tokenizer_.train_from_iterator(acquire_corpus(), trainer=trainer)</span><br><span class="line">    <span class="comment"># post processing for bert input templates</span></span><br><span class="line">    sep_id, cls_id = tokenizer_.token_to_id(<span class="string">'[SEP]'</span>), tokenizer_.token_to_id(<span class="string">''</span>)</span><br><span class="line">    tokenizer_.post_processor = processors.TemplateProcessing(</span><br><span class="line">        single=<span class="string">'[CLS]:0 $A:0 [SEP]:0'</span>,</span><br><span class="line">        pair=<span class="string">'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1'</span>,</span><br><span class="line">        special_tokens=[(<span class="string">'[SEP]'</span>, <span class="string">'[CLS]'</span>)]</span><br><span class="line">    )</span><br><span class="line">    tokenizer_.save(<span class="string">'tokenizer.json'</span>)</span><br></pre></td></tr></table></figure>

<p>we can load the saved tokenizer with :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_tokenizer</span><span class="params">(tokenizer, name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">'wordpeace'</span>:</span><br><span class="line">        tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">            tokenizer_object=tokenizer,</span><br><span class="line">            unk_token=<span class="string">'[UNK]'</span>,</span><br><span class="line">            pad_token=<span class="string">'[PAD]'</span>,</span><br><span class="line">            cls_token=<span class="string">'[CLS]'</span>,</span><br><span class="line">            sep_token=<span class="string">'[SEP]'</span>,</span><br><span class="line">            mask_token=<span class="string">'[MASK]'</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> name == <span class="string">'BPE'</span>:</span><br><span class="line">        tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">            tokenizer_object=tokenizer,</span><br><span class="line">            bos_token=<span class="string">'&lt;|endoftext|&gt;'</span>,</span><br><span class="line">            eos_token=<span class="string">'&lt;|endoftext|&gt;'</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure>

<p>Sometimes, we can combine two tokenizers when we don’t have the resource to retrain it. for example, we have two tokenizer for different<br>languages, but we don’t have the corpus.</p>
<ol>
<li>merge vocabulary json file directly</li>
<li>merge merge.txt file<br>some comments:</li>
<li>this tokenizer may uderperform due to we don’t have the statistics inforamtion of words.</li>
<li>the merge.txt is a priorty list, defining the merge order in Sequence.</li>
<li>we could also add new tokens by tokenizer.add_new_tokens()</li>
</ol>
<h2 id="BPE-in-sentencepiece"><a href="#BPE-in-sentencepiece" class="headerlink" title="BPE in sentencepiece"></a>BPE in sentencepiece</h2><p>In sentencepiece, each tokenizer is a model class defined under GoogleProtobuf. we can combine different tokenizer By<br>serialize/deserialize the original model and add the pieace to new object.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_sentencepiece</span><span class="params">(model_path)</span>:</span></span><br><span class="line">    <span class="comment"># load old model</span></span><br><span class="line">    model = Model.ModelProto()</span><br><span class="line">    model.PraseFromString(open(model_path, <span class="string">'rb'</span>).read())</span><br><span class="line">    <span class="comment"># add new tokens into sentencepiece</span></span><br><span class="line">    special_tokens = [<span class="string">'&lt;pad&gt;'</span>]</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> special_tokens:</span><br><span class="line">        new_token = Model.ModelProto().Sentencepiece()</span><br><span class="line">        new_token.piece = token</span><br><span class="line">        new_token.score = <span class="number">0</span></span><br><span class="line">        model.pieces.append(new_token)</span><br><span class="line">    <span class="comment"># save model</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'new.model'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(model.SerializedToString())</span><br></pre></td></tr></table></figure>


<p> Note: When we change the tokenizers, we shoud change bot the word embedding weights and Languagehead also. we can do this by using the API from HugginbgFace:<br> <a href="https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings" target="_blank" rel="noopener">resize_token_embeddings()</a></p>
<h1 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h1><p><a href="https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0" target="_blank" rel="noopener">https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0</a> </p>
<p><a href="https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3" target="_blank" rel="noopener">https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3</a></p>
<p><a href="https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#building-a-bpe-tokenizer-from-scratch" target="_blank" rel="noopener">https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#building-a-bpe-tokenizer-from-scratch</a></p>
<p><a href="https://github.com/huggingface/tokenizers/issues/690" target="_blank" rel="noopener">https://github.com/huggingface/tokenizers/issues/690</a></p>
<p><a href="https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb" target="_blank" rel="noopener">https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb</a></p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Large-Language-Models/">Large Language Models</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2023/10/18/LLMs-tuning/"><span>Conclusion for Fine-Tuning on LLMs</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/yao2jiang/blog_images/master/bolg_index/header.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2023 By Conglei Xu</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://xiaojianhai.github.io/">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>